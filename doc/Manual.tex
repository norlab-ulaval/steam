%=====================
% ASRL Tech note template
% TD Barfoot, June 2012
% Modified: Sean Anderson, Sept 2012
%=====================

\documentclass[10pt,letterpaper,fleqn,oneside]{article}

% Packages
\usepackage{sectsty}
\usepackage{caption}
\usepackage{times}
\usepackage{amssymb,amsfonts,amsmath,amscd}
\usepackage[pdftex,colorlinks]{hyperref}
\usepackage[pdftex]{graphicx}
\usepackage{fnpos}
\usepackage{subfigure}
\usepackage{verbatim}
\usepackage{fancyhdr}
\usepackage[numbers]{natbib}
\usepackage{microtype}

\usepackage{listings}
\usepackage{xcolor}
\lstset { %
	language=C++,
	backgroundcolor=\color{black!5}, % set backgroundcolor
	basicstyle=\footnotesize,% basic font setting
}

% Math definitions
\input{content_extra/notation_header}

% Custom commands

\newcommand{\code}[1]{\texttt{#1}}

% General page formatting
\vfuzz2pt 
\hfuzz2pt 
\addtolength{\hoffset}{-1.0in} \addtolength{\voffset}{-0.75in}
\setlength{\textwidth}{7in} \setlength{\textheight}{8.25in}
\setlength{\headheight}{0.6in}
\setlength{\headsep}{0.4in}
\setlength{\footskip}{40pt}
\setlength{\fboxsep}{12pt}
\setlength{\parskip}{3pt}
\makeFNbottom \makeFNbelow


% - User Document Input --------------------------------------------------

\newcommand{\UTIASprogram}{N/A}
\newcommand{\UTIAStitle}{STEAM Engine}
\newcommand{\UTIASdocument}{ASRL-2015-SWA001}
\newcommand{\UTIASrevision}{Rev: 1.0}
\newcommand{\UTIASauthor}{S.~Anderson}
\newcommand{\UTIASreviewer}{}

% ---------------------------------------------------------------

% PDF setup
\hypersetup{%
    pdftitle={\UTIASdocument: \UTIAStitle},
    pdfauthor={\UTIASauthor},
    pdfkeywords={},
    pdfsubject={\UTIASprogram},
    pdfstartview=FitH,%
    bookmarks=true,%
    bookmarksopen=true,%
    breaklinks=true,%
    colorlinks=true,%
    linkcolor=blue,anchorcolor=blue,%
    citecolor=blue,filecolor=blue,%
    menucolor=blue,pagecolor=blue,%
    urlcolor=blue
}%

% Define some hyphenation
\hyphenation{aero-space} \hyphenation{auton-omous} \hyphenation{time-stamp}

% ---------------------------------------------------------------

\title{\sf\bfseries \UTIAStitle }

\author
{%
       Sean Anderson \\
       University of Toronto \\
       Institute for Aerospace Studies \\
       \texttt{<sean.anderson@mail.utoronto.ca>}%
}

\date{}

% Where to look for graphics so you don't have to give the directory every time
\graphicspath{{figs/}}

\begin{document}

% Define the basic page style
\fancypagestyle{plain}{%
    \fancyhf{}%
    \fancyfoot[C]{}%
    \fancyhead[R]{\begin{tabular}[b]{r}\small\sf \UTIASdocument\\
         \small\sf\UTIASrevision\\
         \small\sf\today \end{tabular}}%
         \fancyhead[L]{\includegraphics[height=0.45in]%
        {figs/utias.pdf} \begin{tabular}[b]{l}\sc{University of Toronto} \\ \sc{Institute for Aerospace Studies} \\ \mbox{}
     \end{tabular}}%
    \renewcommand{\headrulewidth}{0pt}
    \renewcommand{\footrulewidth}{0pt}%
}

% section, caption fonts, pagestyle
\pagestyle{fancy}
\allsectionsfont{\sf\bfseries}
\renewcommand{\captionlabelfont}{\sf\bfseries}
\reversemarginpar
\renewcommand{\labelitemi}{--}
\renewcommand{\labelitemii}{--}
\renewcommand{\labelitemiii}{--}

% - Set the page headers and footers ----------------------------

\lhead{ \includegraphics[height=0.45in]%
        {figs/utias.pdf} \begin{tabular}[b]{l}\sc{University of Toronto} \\ \sc{Institute for Aerospace Studies} \\ \mbox{} \end{tabular}}
\rhead{ \begin{tabular}[b]{r}\small\sf \UTIASdocument\\
        \small\sf\UTIASrevision\\
        \small\sf\today \end{tabular}}
\chead{}
\lfoot{}
\cfoot{\thepage}
\rfoot{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}


% - User Input Document Text ------------------------------------

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%                  Section
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}

This document serves as an introduction to the Simultaneous Trajectory Estimation and Mapping (STEAM) Engine software.
STEAM Engine is an optimization library aimed at solving batch nonlinear optimization problems involving both SO(3)/SE(3) and continuous-time components. 
This is accomplished by using an iterative Gauss-Newton-style estimator in combination with techniques developed and used by ASRL. 
With respect to SO(3) and SE(3) components, we make use of the constraint sensitive perturbation schemes discussed in \citet{barfoot_tro14}.
STEAM Engine is by no means intended to be the \emph{fastest car on the track};
the intent is simply to be \emph{fast enough} for the types of problems we wish to solve, while being both readable and easy to use by people with a basic background in robotic state estimation.

In this note, we will introduce some terminology, the types of problems we wish to solve, and the methods we use to go about solving them. Note that terms in \code{ThisFont} reflect classes in the code base.
The following sections describe the two major phases involved with setting up and solving an optimization problem with STEAM Engine: i) creating an \code{OptimizationProblem} and ii) passing the problem to a \emph{solver} (such as the \code{LevMarqGaussNewtonSolver}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%                  Section
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\code{OptimizationProblem}}
\label{sec:opt_prob}

The first step in using STEAM Engine is to create an instance of an \code{OptimizationProblem} which fully defines the problem you wish to solve;
this is accomplished by creating and adding the relevant \code{StateVariable} and \code{CostTerm} objects.
STEAM Engine is setup to solve nonlinear optimization problems of the form:
%
\begin{equation}
\label{eq:problem}
\mbfhat{x} = \arg\min_\mbf{x} J(\mbf{x}),
\end{equation}
%
where $J(\mbf{x})$ is the \emph{objective function} or \emph{cost} (composed of many \code{CostTerms}) which we wish to minimize with respect to our \code{StateVector}, $\mbf{x}$. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                Subsection
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\code{CostTerm}}
\label{sec:cost_term}

A typical \emph{objective function}, $J(\mbf{x})$, is composed of many \code{CostTerms}, $J_i(\mbf{x})$:
%
\begin{equation}
\label{eq:cost_term}
J(\mbf{x}) := \sum_i J_i(\mbf{x}), \quad J_i(\mbf{x}) := \rho(e_i), \qquad e_i := ||\mbf{e}_i(\mbf{x})||_{\mbf{R}_i} = \sqrt{\mbf{e}_i(\mbf{x})^T \mbf{R}_i^{-1} \mbf{e}_i(\mbf{x})}.
\end{equation}
%
where each \code{CostTerm}, $J_i(\mbf{x})$, is subsequently composed of i) an \code{ErrorEvaluator} (or \emph{error function}), $\mbf{e}_i(\mbf{x})$, ii) a \code{NoiseModel}, $\mbf{R}_i$, and iii) a \code{LossFunction}, $\rho(e_i)$.

%                Subsubsection
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\code{ErrorEvaluator}}
\label{sec:err_eval}

The purpose of the \code{ErrorEvaluator} is to use the current value of the state variables, $\mbf{x}$, to evaluate the function $\mbf{e}_i(\mbf{x})$; furthermore, \code{ErrorEvaluators} must be equipped with the ability to provide Jacobians with respect to relevant state variables, $\mbf{x}$, given their current value.
Note that as the state variables change value between iterations, so does the output of an evaluator; the concept of \emph{evaluators} are a core part of how STEAM Engine functions, and offer a lot of convenience.

\noindent The standard \code{ErrorEvaluator} assumes the form
%
\begin{equation}
\mbf{e}_i(\mbf{x}) := \mbf{f}_i(\mbf{x}) + \mbf{n}_i, \qquad \mbf{n}_i \sim \N(\mbf{0}, \mbf{R}_i),
\end{equation}
%
where $\mbf{f}_i(\cdot)$ is a (non)linear function which returns a vector-space \emph{cost} or \emph{error} and $\mbf{n}_i$ is zero-mean Gaussian measurement noise with covariance $\mbf{R}_i$ (set through the \code{NoiseModel}).

%                Subsubsection
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\code{NoiseModel}}
\label{sec:noise_model}

The purpose of the \code{NoiseModel} is simply to store the \emph{covariance} or \emph{noise} associated with an \code{ErrorEvaluator}. While a \code{NoiseModel} can be set by either its covariance ($\mbf{R}_i$), information ($\mbf{R}_i^{-1}$), or (upper-triangular) square root information ($\mbf{U}_i$), we internally store the later, for efficiency in \emph{error whitening}.
Note that
%
\begin{equation}
\mbf{U}_i^T \mbf{U}_i = \mbf{R}_i^{-1},
\end{equation}
%
where $\mbf{U}_i$ is an upper-triangular matrix found using \emph{cholesky decomposition}.
\emph{Error whitening} or the \emph{whitening transformation} refers to the process by which we \emph{normalize} the input vector into a white noise vector (given its `known' covariance matrix).
The resultant \emph{whitened errors} can be assumed to have an identity covariance matrix, noting that
%
\begin{equation}
\mbf{e}_i(\mbf{x})^T \mbf{R}_i^{-1} \mbf{e}_i(\mbf{x}) = 
 \mbf{e}_i(\mbf{x})^T \mbf{U}_i^T \mbf{U}_i \mbf{e}_i(\mbf{x}) =
(\mbf{U}_i \mbf{e}_i(\mbf{x}))^T \hspace{-5px}\underbrace{\mbf{U}_i \mbf{e}_i(\mbf{x})}_{\text{whitened error}}.
\end{equation}
%

%                Subsubsection
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\code{LossFunctions}}
\label{sec:loss_func}

Lastly, the use of \code{LossFunctions}, $\rho(\cdot)$, provide users the ability to use \emph{M-estimation} or \emph{robust estimation} techniques on a per-\code{CostTerm} basis.
In the context of solving equation \eqref{eq:problem} using \emph{M-estimation}, we note that it is convenient to rewrite the \code{CostTerm} in \eqref{eq:cost_term} as the equivalent \emph{iteratively reweighted least-squares} (IRLS) term
%
\begin{equation}
\label{eq:irls_cost_term}
J_i(\mbf{x}) = \rho(e_i) = \frac{1}{2} w(e_i) e_i^2 = \frac{1}{2} w(e_i) \, \mbf{e}_i(\mbf{x})^T \mbf{R}_i^{-1} \mbf{e}_i(\mbf{x}),
\end{equation}
%
where we note that a valid \code{LossFunction}, $\rho(e_i)$, has an associated weighting function, $w(e_i)$.
While there are many valid \code{LossFunctions}, the simplest (and arguably most common) example is the least squares (or $L_2$) loss function, for which we note that $\rho(e_i) = \frac{1}{2} e_i^2$ and $w(e_i) = 1$.
Using a $L_2$ \code{LossFunction} for all of the \code{CostTerms}, we are left with the (hopefully) familiar optimization problem,
%
\begin{equation}
\mbfhat{x} = \arg\min_\mbf{x} \frac{1}{2} \sum_i \mbf{e}_i(\mbf{x})^T \mbf{R}_i^{-1} \mbf{e}_i(\mbf{x}).
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                Subsection
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\code{StateVariable}}
\label{sec:state_variable}

The goal of the solver is to minimize the total cost of a collection of \code{CostTerms} with respect to the values of some collection of \code{StateVariables}.
The state variables are therefore typically the desired output of the optimization problem, and given some initial condition, it is the solvers job to try and find a set of values for the state variables which produces a minimum cost.
Note that while the goal is to find the \emph{globally} minimal cost, we may instead find a \emph{locally} minimal cost (depending on the initial condition).
In STEAM Engine, \code{StateVariable} types are created by defining a \emph{value} class (such as a vector class, transformation matrix class, etc.), a \emph{perturbation} dimension, and a method to update the state variable given a vector of the \emph{perturbation} dimension.
Some common types have been implemented and are described in the following subsections.

%                Subsubsection
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\code{VectorSpaceStateVar}}
\label{sec:vec}

Probably the simplest type of state variable, the vector-space state variable, $\mbf{v} \in \mathbb{R}^N$, wraps the \code{Eigen::VectorXd} class, and defines the update method
%
\begin{equation}
\label{eq:vs_updates}
\mbf{v} \leftarrow \mbf{v} + \delta\mbf{v},
\end{equation}
%
where $\delta\mbf{v}$ is the perturbation (with matching dimension $N$) the solver will iterative find and use to update the state variable $\mbf{v}$.

%                Subsubsection
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\code{LieGroupStateVar}}
\label{sec:lg}

Particularly in robotics, it is common to have rotation and transformation matrices as state variables that describe the orientation or pose of the robot in 3D. Note that rotation matrices, $\mbf{C}$, belong to the special orthogonal group, $SO(3)$, and transformation matrices, $\mbf{T}$, belong the special Euclidean group, $SE(3)$.

It is fairly common knowledge how rotation and transformation matrices can be used in practice. However, it is less obvious how we should linearize \code{CostTerms} with respect to them, solve for an update perturbation, and apply that perturbation to the Lie group state variable.
While there exists many parameterizations for rotation matrices, we favour the constraint sensitive perturbation schemes detailed in \citet{barfoot_tro14}.

\noindent The \code{LieGroupStateVar} implementation assumes that Lie group state variables have a multiplicative update, such as
%
\begin{subequations}
\label{eq:lg_updates}
\begin{align}
\mbf{T} \leftarrow& \exp(\delta\mbs{\xi}^\wedge) \mbf{T}, \quad \mbs{\xi} \in \mathbb{R}^6, \\
\mbf{C} \leftarrow& \exp(\delta\mbs{\phi}^\wedge) \mbf{C}, \quad \mbs{\phi} \in \mathbb{R}^3,
\end{align}
\end{subequations}

%
where $\delta\mbs{\xi}$ and $\delta\mbs{\phi}$ are perturbation vectors that would be used to update the matrices $\mbf{T}$ and $\mbf{C}$.
While STEAM Engine does not tie users to a specific transformation matrix implementation, it does assume that the implementations will have constructors based on the exponential map. If this is not suitable, users can easily implement their own Lie group state variable classes based on other update schemes.

\noindent For those interested in more detail on the exponential map, and how uncertainties can be handled, we suggest reading \citet{barfoot_tro14}. 
Here, we provide a brief overview of the closed-form construction of a transformation matrix,
%
\begin{align}
\mbf{T} :=& \exp(\mbs{\xi}^\wedge) = \bbm \mbf{C} & \mbf{J}\mbs{\rho} \\ \mbf{0}^T & 1 \ebm
\end{align}
%
where $\mbs{\xi} \in \mathbb{R}^6$, the $\wedge$ operator turns $\mbs{\xi}$ into a $4 \times 4$ member of the \emph{Lie algebra} \algse,
%
\begin{align}
\mbs{\xi}^\wedge := \bbm \mbs{\rho} \\ \mbs{\phi} \ebm^\wedge 
= \bbm \mbs{\phi}^\wedge & \mbs{\rho} \\ \mbf{0}^T & 0 \ebm, \quad \mbs{\rho}, \mbs{\phi} \in \mathbb{R}^3
\end{align}
%
and $\wedge$ also turns $\mbs{\phi}$ into a $3 \times 3$ member of the \emph{Lie algebra} \algso,
%
\begin{align*}
\mbs{\phi}^\wedge := \bbm \phi_1 \\ \phi_2 \\ \phi_3 \ebm^\wedge := \bbm 0 & -\phi_3 & \phi_2 \\ \phi_3 & 0 & -\phi_1 \\ -\phi_2 & \phi_1 & 0 \ebm.
\end{align*}
%
The rotation matrix, $\mbf{C} \in SO(3)$, can be computed using the exponential map,
%
\begin{align}
\mbf{C} :=& \exp(\mbs{\phi}^\wedge) = \cos\phi \mbf{1} + (1 - \cos\phi) \mbf{a}\mbf{a}^T + \sin\phi\mbf{a}^\wedge,
\end{align}
%
where $\phi = ||\mbs{\phi}||$ is the angle of rotation, and $\mbf{a} = \mbs{\phi}/\phi$ is the axis of rotation.
Lastly, we also have a closed form expression for $\mbf{J}$, which is the (left) Jacobian of $SO(3)$,
%
\begin{align}
\mbf{J} :=& \dfrac{\sin\phi}{\phi} \mbf{1} + \biggl(1 - \dfrac{\sin\phi}{\phi} \biggr) \mbf{a}\mbf{a}^T + \dfrac{1 - \cos\phi}{\phi} \mbf{a}^\wedge.
\end{align}


%                Subsubsection
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\code{LandmarkStateVar}}
\label{sec:landmark}

Landmarks are a very specific type of state used in \emph{vision}-based estimation problems, such as \emph{visual odometry} or \emph{bundle adjustment}.
We implement landmarks as their own type of state variable, rather than a \code{VectorSpaceStateVar}, for two conveniences.
First, we internally store landmarks as a homogeneous coordinate point to make multiplication with transformations more efficient.
Second, we also provide the ability to denote a \emph{reference frame}, which is a transformation matrix, or pose, that the landmark is estimated with respect to.
For example, in the \emph{global} bundle adjustment paradigm we would choose the reference frame to the `base frame' or first camera frame, where as in the \emph{relative} bundle adjustment paradigm we could set each landmark's reference frame to be the pose of the camera at the first time the landmark was observed.

%                Subsubsection
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Locking a \code{StateVariable}}
\label{sec:lock}

The \code{StateVariableBase} class implements a simple method allowing users to lock/unlock state variables.
The expected behaviour of a \emph{locked} state variable is that its value is fixed, and cannot be updated, furthermore, \code{ErrorEvaluators} \emph{should not} create Jacobians with respect to \emph{locked} state variables when evaluated.
The benefit of this feature is that it provides users some reusability with respect to creating \code{StateVariables} and \code{CostTerms}.
Specifically, we note that \code{CostTerms} do not need to be recreated when they depend on two or more state variables (e.g. a transformation matrix and landmark variable) and one of them is locked (e.g. the transformation matrix in a sliding-window-filter-style algorithm).

Note that locked state variables can be added to \code{CostTerms}, but should not be added directly to the \code{OptimizationProblem} as a state variable.
Also, state variables \emph{should not} be locked after having added them to an \code{OptimizationProblem}. It is quick to recreate an \code{OptimizationProblem} by reusing the unlocked \code{StateVariables} and existing \code{CostTerms}.


%                Subsection
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Continuous-Time Trajectories}

TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%                  Section
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Solvers (derivatives of \code{SolverBase})}
\label{sec:solvers}

As a preface, the purpose of this section is to give some intuition behind what the various types of solvers are doing `behind the curtain'. 
General users of the STEAM Engine do not need to know the details of the solvers, and simply need to construct an \code{OptimizationProblem} and pass it to a solver for optimization.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                Subsection
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\code{SolverBase}}

The purpose of the base solver class is to implement the iterative nature of the optimization algorithm and check for stopping criteria.
The default stopping criteria, set through the \code{Params} class are:
%
\begin{enumerate}
	\item Maximum iterations (500)
	\item Absolute cost threshold ($J_\text{new} \leq 0$)
	\item Absolute cost change threshold ($|J_\text{new} - J_\text{old}| \leq 1e-4$)
	\item Relative cost change threshold ($\frac{|J_\text{new} - J_\text{old}|}{J_\text{old}} \leq 1e-4$)
\end{enumerate}
% 
The \code{SolverBase} class is extended by derived classes to implement the linearize, solve and update steps.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                Subsection
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\code{GaussNewtonSolverBase}}

In the current iteration of the STEAM Engine software, the \code{GaussNewtonSolverBase} class is the sole derivative of \code{SolverBase} and implements the basic functionalities needed by the Gauss-Newton-style optimization schemes.

\noindent Recall the IRLS problem:
%
\begin{equation*}
\mbfhat{x} = \arg\min_\mbf{x} J, \qquad J = \frac{1}{2} \sum_i w(e_i) \mbf{e}_i(\mbf{x})^T \mbf{R}_i^{-1} \mbf{e}_i(\mbf{x}).
\end{equation*}
%
Taking the standard Gauss-Newton optimization approach to solving this problem requires us to linearize the \code{ErrorEvaluators} about our best guess of the state, $\mbfbar{x}$,
%
\begin{equation}
\mbf{e}_i(\mbf{x}) \approx \mbf{f}_i(\mbfbar{x}) + \mbf{F}_i \delta\mbf{x}, \quad 
\mbf{F}_i = \dfrac{\partial \mbf{f}}{\partial \mbf{x}}\biggr\rvert_{\mbfbar{x}},
\end{equation}
%
where $\mbf{F}_i$ is a set of \code{Jacobians} (each \code{Jacobian} being related to a single state variable that impacts the \code{ErrorEvaluator}).
Plugging the linearized error terms into the cost function, taking the derivative of the cost with respect to $\delta\mbf{x}$, and setting it to zero, reveals that we must solve the following system of linear equations at each iteration of Gauss-Newton:
%
\begin{equation}
\label{eq:irls_Axb}
\biggl(\sum_i w(e_i) \mbf{F}_i^T \mbf{R}_i^{-1} \mbf{F}_i \biggr) \delta\mbf{x}^* = \sum_i w(e_i) \mbf{F}_i^T \mbf{R}_i^{-1} \mbf{f}_i(\mbfbar{x}),
\end{equation}
%
where $\delta\mbf{x}^*$ is the optimal perturbation.
Solving for $\delta\mbf{x}^*$ and using the result to update $\mbfbar{x}$ according to our update rules (recall \eqref{eq:vs_updates} and \eqref{eq:lg_updates}), we can iteratively solve the problem in \eqref{eq:problem}.

Note that the \code{GaussNewtonSolverBase} class simply implements methods for building the left-hand-side and right-hand-side terms of \eqref{eq:irls_Axb} using block-sparse matrices (note the build step is parallelized with OpenMP, the number of threads to use can be set in the CMakeLists file), and then solving the system for $\delta\mbf{x}^*$ with a sparse \code{Eigen} LLT decomposition implementation.
It also provides some methods needed by the Levenberg-Marquardt and Powell's Dogleg solvers based on the terms of \eqref{eq:irls_Axb}.
Derivatives of this class are responsible for actually updating the state, as various algorithms, such as trust-region solvers, have different approaches to applying the update $\delta\mbf{x}^*$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                Subsection
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\code{VanillaGaussNewtonSolver}}

The \emph{vanilla} Gauss-Newton solver uses the basic method described above to solve for $\delta\mbf{x}^*$ and \emph{blindly} updates $\mbfbar{x}$. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                Subsection
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Line Search and Trust Region Solvers}

A solver's \emph{update}, or \emph{step}, strategy involves both a \emph{step} direction and a \emph{step} length. 
In the context of Gauss-Newton-style solvers, there are two major paradigms in step strategy: i) line searches and ii) trust regions. 

Line searches typically work on a fixed step direction and then search for a step length that reduces the cost, $J$. 
In contrast, trust region methods tend to set a \emph{region size} (usually related to step length), for which it believes the linearization well approximates the nonlinear cost function, and then chooses a step direction; 
note that the step length is adjusted dynamically as the trust region becomes too conservative, or not conservative enough.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                Subsection
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\code{LineSearchGaussNewtonSolver}}

This solver implements a basic backtracking line search.
Essentially, the algorithm tries updating the state using the Gauss-Newton step, $\delta\mbf{x}^*$, and then evaluates the \code{CostTerms} to see if $J_\text{new}$ is less than  $J_\text{old}$.
If it is not, then the state is reverted, the step is reduced by the scalar \emph{backtrack multiplier} (default $0.5$) and the update is tested again (note the `direction' is the same).
This procedure repeats until the cost is reduced or a maximum number of backtracks are tried (default 10).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                Subsection
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\code{LevMarqGaussNewtonSolver}}

The Levenberg-Marquardt trust-region method is one which smoothly transitions between the Gauss-Newton and gradient-descent steps based on how much it \emph{trusts} the step.
As an aside, the Gauss-Newton step direction has very strong convergence when the linearization well approximates the nonlinear function, but is not guaranteed to reduce the cost. 
In contrast, the gradient-descent direction generally has slow convergence, but can be trusted to provide a direction that (locally) reduces the cost.
Recalling that \eqref{eq:irls_Axb} has the form, $\mbf{A} \delta\mbf{x}^* = \mbf{b}$, the Levenberg-Marquardt method solves the augmented system of equations:
%
\begin{equation}
(\mbf{A} + \lambda \diag(\mbf{A})) \, \delta\mbf{x}^* = \mbf{b},
\end{equation}
%
where $\lambda$ is the trust-region parameter.
As $\lambda$ tends to zero, we solve for the Gauss-Newton step direction, and as $\lambda$ increases, we tend towards the gradient-descent direction.

Similar to the line-search methodology, after solving for a step update, $\delta\mbf{x}^*$, the effect on cost reduction is tested, and depending on the result, the update is either accepted or rejected, and the trust-region parameter, $\lambda$, is modified accordingly.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                Subsection
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\code{DoglegGaussNewtonSolver}}

The Powell's Dogleg algorithm is an alternative trust-region solver (similar in nature to the Levenberg-Marquardt method) that uses some simple heuristics to choose a step direction and length.
In essence, the Dogleg algorithm interpolates the step direction and length from the intersection of the `trust-region size', $\lambda$, with one of two discrete `paths'.
The first `path' extends from zero along the gradient-descent direction to what is known as the \emph{Cauchy} point:
%
\begin{equation}
\mbf{c} = \dfrac{\mbf{b}^T \mbf{b}}{\mbf{b}^T \mbf{A} \mbf{b}}.
\end{equation}
%
The second `path' connects the \emph{Cauchy} point, $\mbf{c}$, and the Gauss-Newton step, $\delta\mbf{x}^*$.
Depending on the size of $\lambda$, we interpolate the step length and direction from one of the two paths, or if $\lambda$ is greater than the Euclidean norm of the Gauss-Newton step, $\delta\mbf{x}^*$, then we simply take the Gauss-Newton step.

This method generally performs very well, even in contrast with the more sophisticated Levenberg-Marquardt method.
Furthermore, the Powell's Dogleg algorithm is very fast because it does not need to resolve the system of equations when $\lambda$ changes (unlike Levenberg-Marquardt).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%                  Section
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\code{TransformEvaluators}}
\label{sec:evals}

\code{TransformEvaluators} are a structure which allows us to abstract multiple operations which ultimately result in a single pose change (and allow us to keep track of how to take Jacobians with respect to any states involved in those operations). 
This is a major convenience tool for those who are unfamiliar with the \emph{Lie group} maths and are new to taking Jacobians with respect to the se(3) perturbations. 
The other major benefit of \code{TransformEvaluators} is that it provides a high level of reusability when it comes to creating \code{ErrorEvaluators}.

\subsection{Stereo Camera Error Metric}

To demonstrate how \code{TransformEvaluators} can be used, and how they make code reusable, we provide this example.
First we will derive the necessary maths, then show how we might `hardcode' an \code{ErrorEvaluator}, and finally how it can be done using \code{TransformEvaluators}.

\subsubsection{Nonlinear Measurement}

We begin by considering a simple visual measurement model for a stereo-camera,
%
\begin{equation}
\mbf{y}_i = \mbf{h}(\mbf{T}_{i,0} \mbf{p}_\ell) + \mbf{n}_i, \quad 
\mbf{n}_i \sim \N(\mbf{0}, \mbf{R}_i), \quad
\mbf{p}_\ell = \bbm \mbs{\zeta}_0^{\ell,0} \\ 1 \ebm,
\end{equation}
%
where $\mbf{T}_{i,0}$ is the pose of the camera at timestep $i$ with respect to base frame $0$, $\mbf{p}_\ell$ is the homogeneous point coordinate for landmark $\ell$, and $\mbf{h}(\cdot)$ is the nonlinear stereo-camera model that transforms a 3D point into the stereo pixel coordinates:
%
\begin{equation}
\mbf{h}(\mbf{p}) 
= \bbm u_l \\ v_l \\ u_r \\ v_r \ebm
= \frac{1}{z} \bbm f_u x \\ f_v y \\ f_u (x-b) \\ f_v y \ebm 
  + \bbm c_u \\ c_v \\ c_u \\ c_v \ebm
\end{equation}
%
where $x$, $y$, and $z$ are coordinates of the point $\mbf{p}$, $f_u$ and $f_v$ are the horizontal and vertical focal lengths, $b$ is the stereo baseline parameter, and $c_u$ and $c_v$ are the cameras horizontal and vertical optical centers.

\noindent Formulating this into an error term, we typically write
%
\begin{equation}
\label{eq:stereo_error}
\mbf{e}_i = \mbf{y}_i - \mbf{h}(\mbf{T}_{i,0} \mbf{p}_\ell)
\end{equation}
%
where $\mbf{y}_i$ is a physical observation and $\mbf{e}_i$ is the error based on our estimate of the state variables $\mbf{T}_{i,0}$ and $\mbf{p}_\ell$.
In order to linearize this error term, we begin by noting that \eqref{eq:stereo_error} has a composition of two nonlinearities, one for the camera model and one for transformating the landmark into the sensor frame.
We separate the two nonlinearities by defining
%
\begin{equation}
\mbf{g}_i := \mbf{T}_{i,0} \mbf{p}_\ell.
\end{equation}
%
Applying our perturbation schemes, we then have
%
\begin{equation}
\mbf{g}_i = \exp(\delta\mbs{\xi}_{i,0}^\wedge) \mbf{T}_{i,0} (\mbf{p}_\ell + \mbf{D} \delta\mbs{\zeta}_\ell), \quad
\text{where} \; \mbf{D} := \bbm 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \ebm.
\end{equation}
%
Noting that when $\delta\mbs{\xi}$ is small, 
%
\begin{equation}
\exp(\delta\mbs{\xi}^\wedge) \approx \mbf{1} + \delta\mbs{\xi}^\wedge,
\end{equation}
%
and using the identity
\begin{equation}
\mbf{w}^\wedge \mbf{y} \equiv \mbf{y}^\odot \mbf{w}, \quad
\mbf{y}^\odot := \bbm \mbs{\varepsilon} \\ \eta \ebm^\odot = \bbm \eta \mbf{1} & -\mbs{\varepsilon}^\wedge \\ \mbf{0}^T & \mbf{0}^T \ebm,
\end{equation}
%
we are able to expand drop small terms to find that,
%
\begin{align}
\mbf{g}_i \approx& (\mbf{1} + \delta\mbs{\xi}_{i,0}^\wedge) \mbf{T}_{i,0} (\mbf{p}_\ell + \mbf{D} \delta\mbs{\zeta}_\ell) \\
%
=& \mbf{T}_{i,0} \mbf{p}_\ell + (\mbf{T}_{i,0} \mbf{p}_\ell)^\odot \delta\mbs{\xi}_{i,0} + \mbf{T}_{i,0} \mbf{D} \delta\mbs{\zeta}_\ell \\
%
=& \mbfbar{g}_i + \mbf{G}_i \delta\mbf{x} \bbm \delta\mbs{\xi}_{i,0} \\ \delta\mbs{\zeta}_\ell \ebm,
\end{align}
%
correct to first order, where
%
\begin{subequations}
\begin{align}
\mbfbar{g}_i :=&  \mbf{T}_{i,0} \mbf{p}_\ell, \\
\mbf{G}_i :=& \bbm (\mbf{T}_{i,0} \mbf{p}_\ell)^\odot & \mbf{T}_{i,0} \mbf{D} \ebm.
\end{align}
\end{subequations}
%
%
Returning to our measurement error term,
%
\begin{align}
\mbf{e}_i &\approx \mbf{y}_i - \mbf{h} \biggl( \bar{\mbf{g}}_i + \mbf{G}_i \bbm \delta\mbs{\xi}_{i,0} \\ \delta\mbs{\zeta}_\ell \ebm \biggr) \nonumber\\
%---
&\approx \mbf{y}_i - \mbf{h}( \bar{\mbf{g}}_i) - \mbf{H}_i \mbf{G}_i \bbm \delta\mbs{\xi}_{i,0} \\ \delta\mbs{\zeta}_\ell \ebm \nonumber\\
%---
&= \bar{\mbf{e}}_i + \mbf{F}_i \bbm \delta\mbs{\xi}_{i,0} \\ \delta\mbs{\zeta}_\ell \ebm 
\end{align}
%
correct to first order, where
%
\begin{subequations}
	\begin{align}
	\bar{\mbf{e}}_i &:= \mbf{y}_i - \mbf{h}(\bar{\mbf{g}}_i), \\
	%---
	\mbf{F}_i &:= -\mbf{H}_i \mbf{G}_i, 
	\quad\quad
	\mbf{H}_i := \dfrac{\partial \mbf{h}}{\partial \mbf{g}} \biggr\rvert_{\bar{\mbf{g}}_i}.
	\end{align}
\end{subequations}
%

\subsubsection{Simple \code{ErrorEvaluator}}

\noindent In order to use this error term in our code, we create an \code{ErrorEvaluator}.
The constructor for such a class might look like:
%
\begin{lstlisting}
StereoCameraErrorEval(const Eigen::Vector4d& meas,
                      const CameraIntrinsics::ConstPtr& intrinsics,
                      const se3::TransformStateVar::ConstPtr& T_cam_landmark,
                      const se3::LandmarkStateVar::ConstPtr& landmark);
\end{lstlisting}
%
where the \code{Eigen::Vector4d} holds our measurement, \code{CameraIntrinsics} is a structure that holds the stereo camera intrinsic parameters, and the \code{se3::TransformStateVar} and \code{se3::LandmarkStateVar} are our state variables.
The main functionalities we must then implement are the methods:
%
\begin{lstlisting}
virtual Eigen::VectorXd evaluate() const;
virtual Eigen::VectorXd evaluate(std::vector<Jacobian>* jacs) const;
\end{lstlisting}
%
We will focus on the later, as it encompasses the full functionality:
%
\begin{lstlisting}
Eigen::VectorXd StereoCameraErrorEval::evaluate(std::vector<Jacobian>* jacs) const {

  // Get point in camera frame
  Eigen::Vector4d point_in_c = T_cam_landmark_->getValue() * landmark_->getValue();

  // Get Jacobian for the camera model
  Eigen::Matrix4d H = cameraModelJacobian(point_in_c);
  
  // Construct diffusion matrix
  Eigen::MatrixXd D = Eigen::Matrix<double,4,3>::Identity();

  // Check and initialize jacobian array
  CHECK_NOTNULL(jacs);
  jacs->clear();
  jacs->reserve(2);

  // Calculate Jacobian w.r.t. transform
  if(!T_cam_landmark_->isLocked()) {
    jacs->push_back(Jacobian(T_cam_landmark_->getKey(), 
                             -H * lgmath::se3::point2fs(point_in_c.head<3>())));
  }

  // Calculate Jacobian w.r.t. landmark
  if(!landmark_->isLocked()) {
    jacs->push_back(Jacobian(landmark_->getKey(),
                             -H * T_cam_landmark_->getValue().matrix() * D));
  }

  // Return error (between measurement and point estimate projected in camera frame)
  return meas_ - cameraModel(point_in_c);
}
\end{lstlisting}

Analyzing this code, we encounter a few new concepts to STEAM Engine that are not previously explained.
Firstly, in order to evaluate a state variable, we call \code{getValue()}, as seen in the line that evaluates the point in the sensor frame, \code{point\_in\_c}.
Next, we see the process of creating and returning \code{Jacobians} with respect to unlocked state variables.
Note that if a state variable is locked, STEAM Engine expects that you will not spend the time calculating and returning \code{Jacobian} terms for it.
The two components of a \code{Jacobian} are a \code{StateKey} (found by calling \code{getKey()}) that associate the \code{Jacobian} to a specific instance of a \code{StateVariable}, and the actual matrix value at evaluation time. 
Lastly, we return the error vector.

\subsubsection{Why use \code{TransformEvaluators}}

The primary reason to use \code{TransformEvaluators} is code reusability.
Consider our previous example, but now we have additional sensors (such as an IMU) with extrinsic calibrations.
Instead of our state variables, $\mbf{T}_{i,0}$, representing camera poses, they now represent vehicle poses, and we must use the extrinsic calibration between the camera and vehicle, $\mbf{T}_{c,v}$, to related measured points to our state variables.
To implement this, we would have to modify our previous class to now accept an extrinsic calibration, and make sure that all our \code{Jacobians} are still correct.
If we instead had a constructor that just accepted a \code{TransformEvaluator}:
%
\begin{lstlisting}
StereoCameraErrorEval(const Eigen::Vector4d& meas,
                      const CameraIntrinsics::ConstPtr& intrinsics,
                      const se3::TransformEvaluator::ConstPtr& T_cam_landmark,
                      const se3::LandmarkStateVar::ConstPtr& landmark);
\end{lstlisting}
%
the code becomes very extensible.
Recall, \code{TransformEvaluators} are a structure which allows us to abstract multiple operations which ultimately result in a single pose change.
\code{TransformEvaluators} can be used to encapsulate a transformation state variable
%
\begin{lstlisting}
steam::se3::TransformEvaluator::Ptr pose_v_0 = steam::se3::TransformStateEvaluator::MakeShared(T_i0);
\end{lstlisting}
%
a fixed transformation, such as the extrinsic calibration,
%
\begin{lstlisting}
steam::se3::TransformEvaluator::Ptr pose_c_v = steam::se3::FixedTransformEvaluator::MakeShared(T_cv);
\end{lstlisting}
%
and finally compose them using the provided \code{TranformEvalOperations},
%
\begin{lstlisting}
steam::se3::TransformEvaluator::Ptr T_cam_landmark = steam::se3::compose(pose_c_v, pose_v_0);
\end{lstlisting}
%
The resultant transformation matrix evaluator still has ties to the underlying state variables, and similar to an \code{ErrorEvaluator}, knows how to provide \code{Jacobians} with respect to any state variable transformation matrices involved in its evaluation.

\noindent The available operations (some of which can be chained), with known Jacobians, are:
%
\begin{equation}
\mbf{T}_1 \mbf{T}_2, \quad
\mbf{T}^{-1}, \quad
\mbf{T}\mbf{p}, \quad
\ln(\mbf{T})^\vee.
\end{equation}
%
Using these operations, it becomes trivial to implement new error metrics.
For example, implementing a relative transformation error metric is as simple as:
%
\begin{enumerate}
\item Creating \code{TransformEvaluators} for the state variables, say $\mbf{T}_1$ and $\mbf{T}_2$
\item Creating a \code{TransformEvaluator} for the fixed measurement $\mbfbar{T}_{21}$
\item Using the inverse operation on $\mbf{T}_2$
\item Using the compose operation on $\mbf{T}_1$ with $\mbf{T}_2^{-1}$ to create $\mbf{T}_{12}$
\item Using the compose operation on $\mbfbar{T}_{21}$ with $\mbf{T}_{12}$
\item Using the logarithmic map operation $\ln(\mbfbar{T}_{21} \mbf{T}_{12})^\vee$, which returns a $6 \times 1$ vector
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%                  Section
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{More Examples}

To try out and start understanding the STEAM Engine functionality, take a look at the samples:
\begin{itemize}
	\item TrustRegionExample.cpp
	\item SimplePoseGraphRelax.cpp
	\item SpherePoseGraphRelax.cpp
	\item SimpleBundleAdjustment.cpp
	\item SimpleBundleAdjustmentRelLand.cpp
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%                  Section
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Templating and Polymorphism}
%\label{sec:name}
%
%For the roboticist who does not consider themselves an \emph{expert} programmer, or has only had a brief introduction to C++ during their studies, this section will give a very brief introduction to Templating and Polymorphism (probably the two most understood topics by novice programmers).
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                Subsection
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Templating}
%\label{sec:name}
%
%what it is
%
%simple example (operation on int or float that we can compile for each)
%
%example of where it is used in our code
%
%...
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                Subsection
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Polymorphism}
%\label{sec:name}
%
%what it is
%
%simple example (pets?)
%
%example of where it is used in our code
%
%...


% ---------------------------------------------------------------

% References
%\newpage
\bibliographystyle{refs/asrl}
\bibliography{refs/refs}

\end{document}
